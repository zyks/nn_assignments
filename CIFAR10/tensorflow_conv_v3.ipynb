{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Vendor:  Continuum Analytics, Inc.\n",
      "Package: mkl\n",
      "Message: trial mode expires in 22 days\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import tensorflow as tf\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from fuel.datasets.cifar10 import CIFAR10\n",
    "from fuel.transformers import ScaleAndShift, Cast, Flatten, Mapping\n",
    "from fuel.streams import DataStream\n",
    "from fuel.schemes import SequentialScheme, ShuffledScheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "CIFAR10.default_transformers = (\n",
    "    (ScaleAndShift, [2.0 / 255.0, -1], {'which_sources': 'features'}),\n",
    "    (Cast, [np.float32], {'which_sources': 'features'})\n",
    ")\n",
    "\n",
    "\n",
    "cifar10_train = CIFAR10((\"train\",), subset=slice(None, 40000))\n",
    "cifar10_train_stream = DataStream.default_stream(\n",
    "    cifar10_train,\n",
    "    iteration_scheme=ShuffledScheme(cifar10_train.num_examples, 10)\n",
    ")\n",
    "\n",
    "cifar10_validation = CIFAR10((\"train\",), subset=slice(40000, None))\n",
    "cifar10_validation_stream = DataStream.default_stream(\n",
    "    cifar10_validation, \n",
    "    iteration_scheme=SequentialScheme(cifar10_validation.num_examples, 100)\n",
    ")\n",
    "\n",
    "cifar10_test = CIFAR10((\"test\",))\n",
    "cifar10_test_stream = DataStream.default_stream(\n",
    "    cifar10_test,\n",
    "    iteration_scheme=SequentialScheme(cifar10_test.num_examples, 100)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The streams return batches containing (u'features', u'targets')\n",
      "Each trainin batch consits of a tuple containing:\n",
      " - an array of size (10, 3, 32, 32) containing float32\n",
      " - an array of size (10, 1) containing uint8\n",
      "Validation/test batches consits of tuples containing:\n",
      " - an array of size (100, 3, 32, 32) containing float32\n",
      " - an array of size (100, 1) containing uint8\n"
     ]
    }
   ],
   "source": [
    "print \"The streams return batches containing %s\" % (cifar10_train_stream.sources,)\n",
    "\n",
    "print \"Each trainin batch consits of a tuple containing:\"\n",
    "for element in next(cifar10_train_stream.get_epoch_iterator()):\n",
    "    print \" - an array of size %s containing %s\" % (element.shape, element.dtype)\n",
    "    \n",
    "print \"Validation/test batches consits of tuples containing:\"\n",
    "for element in next(cifar10_test_stream.get_epoch_iterator()):\n",
    "    print \" - an array of size %s containing %s\" % (element.shape, element.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def affine_layer(X, num_hidden, activation=None, name=None):\n",
    "    name = name or \"affine_layer\"\n",
    "    with tf.variable_scope(None, default_name=name):\n",
    "        W = tf.get_variable('W', (X.shape[1].value, num_hidden), 'float32',\n",
    "                            initializer=tf.contrib.layers.xavier_initializer())\n",
    "        b = tf.get_variable('b', (1, num_hidden), 'float32',\n",
    "                            initializer=tf.zeros_initializer())\n",
    "        Y = tf.matmul(X, W) + b\n",
    "        if activation:\n",
    "            Y = activation(Y)\n",
    "        return Y\n",
    "\n",
    "def conv2d_layer(X, filter_shape, num_filters, activation=None, name=None):\n",
    "    name = name or \"conv2d_layer\"\n",
    "    num_channels = X.shape[3].value\n",
    "    with tf.variable_scope(None, default_name=name):\n",
    "        F = tf.get_variable('F', (filter_shape[0], filter_shape[1], num_channels, num_filters), 'float32',\n",
    "                            initializer=tf.contrib.layers.xavier_initializer_conv2d())\n",
    "        b = tf.get_variable('b', (1, 1, 1, num_filters), 'float32',\n",
    "                            initializer=tf.zeros_initializer())\n",
    "        Y = tf.nn.conv2d(X, F, (1, 1, 1, 1), padding='VALID') + b\n",
    "        if activation:\n",
    "            Y = activation(Y)\n",
    "        return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(shape=(None, 3, 32, 32), dtype=np.float32, name='X')\n",
    "X_NHWC = tf.transpose(X, [0, 2, 3, 1])\n",
    "Y = tf.placeholder(shape=(None, 1), dtype=np.int32, name='Y')\n",
    "\n",
    "X_flat = tf.reshape(X, (-1, np.prod(X.shape.as_list()[1:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape (?, 32, 32, 3)\n",
      "After conv shape (?, 28, 28, 32)\n",
      "After pool shape (?, 14, 14, 32)\n",
      "After conv shape (?, 12, 12, 64)\n",
      "After pool shape (?, 6, 6, 64)\n",
      "After conv shape (?, 4, 4, 128)\n",
      "After pool shape (?, 2, 2, 128)\n",
      "After flattening (?, 512)\n",
      "After affine (?, 1000)\n",
      "After affine (?, 1000)\n"
     ]
    }
   ],
   "source": [
    "if 0:\n",
    "    L = X_flat\n",
    "else:\n",
    "    L = X_NHWC\n",
    "    print (\"Input shape %s\" % (L.shape,))\n",
    "    for filter_size, num_filters in [(5, 32), (3, 64), (3, 128)]:\n",
    "        L = conv2d_layer(L, (filter_size, filter_size), num_filters, activation=tf.nn.relu)\n",
    "        print (\"After conv shape %s\" % (L.shape,))\n",
    "        L = tf.nn.max_pool(L, (1, 2, 2, 1), (1, 2, 2, 1), padding='VALID')\n",
    "        L = tf.nn.dropout(L, keep_prob=0.8)\n",
    "        print (\"After pool shape %s\" % (L.shape,))\n",
    "    L = tf.reshape(L, (-1, np.prod(L.shape.as_list()[1:])))\n",
    "    print (\"After flattening %s\" % (L.shape,))\n",
    "    \n",
    "for layer_dim in [1000, 1000]:\n",
    "    L = affine_layer(L, layer_dim, activation=tf.nn.relu)\n",
    "    print (\"After affine %s\" % (L.shape,))\n",
    "    L = tf.nn.dropout(L, keep_prob=0.8)\n",
    "L = affine_layer(L, 10)\n",
    "\n",
    "per_example_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=L, labels=tf.reshape(Y, (-1,)))\n",
    "xentropy_loss = tf.reduce_mean(per_example_loss)\n",
    "\n",
    "weight_decay_loss = 0.0\n",
    "for V in tf.trainable_variables():\n",
    "    if re.match(V.name, \".*/W.*\"):\n",
    "        weight_decay_loss += 1.0e-5 * tf.reduce_sum(V**2)\n",
    "\n",
    "\n",
    "batch_loss = xentropy_loss + weight_decay_loss\n",
    "classification = tf.argmax(L, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "learning_rate = tf.get_variable('learning_rate', shape=(), dtype='float32', trainable=False)\n",
    "global_step = tf.get_variable('global_step', shape=(), dtype='int32', trainable=False)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(batch_loss)\n",
    "train_op = tf.group(train_op, tf.assign_add(global_step, 1))\n",
    "initialize_op = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "sess.run(initialize_op)\n",
    "\n",
    "train_loss_history = []\n",
    "valid_loss_history = []\n",
    "epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.9999997e-05"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(learning_rate.assign(1e-4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, step: 1, loss: 2.34483\n",
      "epoch: 1, step: 1001, loss: 1.90192\n",
      "epoch: 1, step: 2001, loss: 1.70728\n",
      "epoch: 1, step: 3001, loss: 1.46581\n",
      "epoch: 1, step: 4000, valid_loss: 1.50077, valid_acc: 0.457400\n",
      "epoch: 2, step: 4001, loss: 2.03333\n",
      "epoch: 2, step: 5001, loss: 1.88278\n",
      "epoch: 2, step: 6001, loss: 1.57185\n",
      "epoch: 2, step: 7001, loss: 1.17679\n",
      "epoch: 2, step: 8000, valid_loss: 1.29506, valid_acc: 0.539600\n",
      "epoch: 3, step: 8001, loss: 1.03921\n",
      "epoch: 3, step: 9001, loss: 1.01623\n",
      "epoch: 3, step: 10001, loss: 1.02932\n",
      "epoch: 3, step: 11001, loss: 0.895262\n",
      "epoch: 3, step: 12000, valid_loss: 1.20752, valid_acc: 0.575800\n",
      "epoch: 4, step: 12001, loss: 1.53113\n",
      "epoch: 4, step: 13001, loss: 1.33286\n",
      "epoch: 4, step: 14001, loss: 1.13804\n",
      "epoch: 4, step: 15001, loss: 1.0964\n",
      "epoch: 4, step: 16000, valid_loss: 1.15056, valid_acc: 0.593900\n",
      "epoch: 5, step: 16001, loss: 0.706062\n",
      "epoch: 5, step: 17001, loss: 0.725947\n",
      "epoch: 5, step: 18001, loss: 0.873986\n",
      "epoch: 5, step: 19001, loss: 1.36948\n",
      "epoch: 5, step: 20000, valid_loss: 1.0919, valid_acc: 0.615500\n",
      "epoch: 6, step: 20001, loss: 0.879107\n",
      "epoch: 6, step: 21001, loss: 1.07306\n",
      "epoch: 6, step: 22001, loss: 1.96307\n",
      "epoch: 6, step: 23001, loss: 0.868915\n",
      "epoch: 6, step: 24000, valid_loss: 1.03802, valid_acc: 0.638300\n",
      "epoch: 7, step: 24001, loss: 1.11239\n",
      "epoch: 7, step: 25001, loss: 0.879234\n",
      "epoch: 7, step: 26001, loss: 0.78764\n",
      "epoch: 7, step: 27001, loss: 1.0689\n",
      "epoch: 7, step: 28000, valid_loss: 1.0169, valid_acc: 0.638700\n",
      "epoch: 8, step: 28001, loss: 1.45769\n",
      "epoch: 8, step: 29001, loss: 0.854121\n",
      "epoch: 8, step: 30001, loss: 0.952618\n",
      "epoch: 8, step: 31001, loss: 0.466063\n",
      "epoch: 8, step: 32000, valid_loss: 0.970923, valid_acc: 0.657200\n",
      "epoch: 9, step: 32001, loss: 1.18803\n",
      "epoch: 9, step: 33001, loss: 0.701154\n",
      "epoch: 9, step: 34001, loss: 1.27925\n",
      "epoch: 9, step: 35001, loss: 0.451924\n",
      "epoch: 9, step: 36000, valid_loss: 0.956566, valid_acc: 0.666200\n",
      "epoch: 10, step: 36001, loss: 0.783838\n",
      "epoch: 10, step: 37001, loss: 0.697915\n",
      "epoch: 10, step: 38001, loss: 0.490036\n",
      "epoch: 10, step: 39001, loss: 0.764296\n",
      "epoch: 10, step: 40000, valid_loss: 0.92634, valid_acc: 0.676700\n",
      "epoch: 11, step: 40001, loss: 1.09689\n",
      "epoch: 11, step: 41001, loss: 0.502028\n",
      "epoch: 11, step: 42001, loss: 0.832279\n",
      "epoch: 11, step: 43001, loss: 0.763212\n",
      "epoch: 11, step: 44000, valid_loss: 0.91858, valid_acc: 0.679400\n",
      "epoch: 12, step: 44001, loss: 0.779198\n",
      "epoch: 12, step: 45001, loss: 0.695055\n",
      "epoch: 12, step: 46001, loss: 0.772313\n",
      "epoch: 12, step: 47001, loss: 0.367744\n",
      "epoch: 12, step: 48000, valid_loss: 0.905626, valid_acc: 0.684500\n",
      "epoch: 13, step: 48001, loss: 0.614172\n",
      "epoch: 13, step: 49001, loss: 0.725324\n",
      "epoch: 13, step: 50001, loss: 0.992798\n",
      "epoch: 13, step: 51001, loss: 0.726506\n",
      "epoch: 13, step: 52000, valid_loss: 0.892195, valid_acc: 0.691700\n",
      "epoch: 14, step: 52001, loss: 0.793113\n",
      "epoch: 14, step: 53001, loss: 0.396793\n",
      "epoch: 14, step: 54001, loss: 0.375948\n",
      "epoch: 14, step: 55001, loss: 1.05433\n",
      "epoch: 14, step: 56000, valid_loss: 0.880194, valid_acc: 0.696500\n",
      "epoch: 15, step: 56001, loss: 0.49753\n",
      "epoch: 15, step: 57001, loss: 1.65194\n",
      "epoch: 15, step: 58001, loss: 0.258444\n",
      "epoch: 15, step: 59001, loss: 0.48333\n",
      "epoch: 15, step: 60000, valid_loss: 0.860394, valid_acc: 0.708600\n",
      "epoch: 16, step: 60001, loss: 0.857871\n",
      "epoch: 16, step: 61001, loss: 0.248747\n",
      "epoch: 16, step: 62001, loss: 1.04277\n",
      "epoch: 16, step: 63001, loss: 0.931579\n",
      "epoch: 16, step: 64000, valid_loss: 0.858031, valid_acc: 0.707900\n",
      "epoch: 17, step: 64001, loss: 0.327762\n",
      "epoch: 17, step: 65001, loss: 0.593071\n",
      "epoch: 17, step: 66001, loss: 0.419671\n",
      "epoch: 17, step: 67001, loss: 0.808806\n",
      "epoch: 17, step: 68000, valid_loss: 0.843937, valid_acc: 0.715500\n",
      "epoch: 18, step: 68001, loss: 1.15076\n",
      "epoch: 18, step: 69001, loss: 0.908561\n",
      "epoch: 18, step: 70001, loss: 0.487608\n",
      "epoch: 18, step: 71001, loss: 1.29748\n",
      "epoch: 18, step: 72000, valid_loss: 0.867214, valid_acc: 0.706800\n",
      "epoch: 19, step: 72001, loss: 0.601225\n",
      "epoch: 19, step: 73001, loss: 0.737993\n",
      "epoch: 19, step: 74001, loss: 0.475812\n",
      "epoch: 19, step: 75001, loss: 0.212623\n",
      "epoch: 19, step: 76000, valid_loss: 0.860236, valid_acc: 0.709400\n",
      "epoch: 20, step: 76001, loss: 0.760799\n",
      "epoch: 20, step: 77001, loss: 0.596081\n",
      "epoch: 20, step: 78001, loss: 0.649262\n",
      "epoch: 20, step: 79001, loss: 0.630794\n",
      "epoch: 20, step: 80000, valid_loss: 0.855059, valid_acc: 0.720100\n",
      "epoch: 21, step: 80001, loss: 0.830892\n",
      "epoch: 21, step: 81001, loss: 0.518334\n",
      "epoch: 21, step: 82001, loss: 0.651003\n",
      "epoch: 21, step: 83001, loss: 0.352988\n",
      "epoch: 21, step: 84000, valid_loss: 0.852014, valid_acc: 0.713800\n",
      "epoch: 22, step: 84001, loss: 0.559662\n",
      "epoch: 22, step: 85001, loss: 0.976584\n",
      "epoch: 22, step: 86001, loss: 0.399963\n",
      "epoch: 22, step: 87001, loss: 0.149228\n",
      "epoch: 22, step: 88000, valid_loss: 0.857973, valid_acc: 0.721600\n",
      "epoch: 23, step: 88001, loss: 0.500983\n",
      "epoch: 23, step: 89001, loss: 0.31525\n",
      "epoch: 23, step: 90001, loss: 0.885446\n",
      "epoch: 23, step: 91001, loss: 0.218037\n",
      "epoch: 23, step: 92000, valid_loss: 0.865733, valid_acc: 0.716900\n",
      "epoch: 24, step: 92001, loss: 0.383016\n",
      "epoch: 24, step: 93001, loss: 0.327307\n",
      "epoch: 24, step: 94001, loss: 0.534713\n",
      "epoch: 24, step: 95001, loss: 0.52469\n",
      "epoch: 24, step: 96000, valid_loss: 0.846358, valid_acc: 0.724000\n",
      "epoch: 25, step: 96001, loss: 0.453045\n",
      "epoch: 25, step: 97001, loss: 0.594262\n",
      "epoch: 25, step: 98001, loss: 0.169559\n",
      "epoch: 25, step: 99001, loss: 0.109331\n",
      "epoch: 25, step: 100000, valid_loss: 0.851589, valid_acc: 0.723700\n"
     ]
    }
   ],
   "source": [
    "# train for a while\n",
    "global_step_v = sess.run(global_step)\n",
    "\n",
    "while global_step_v < 100000:\n",
    "    epoch += 1\n",
    "    \n",
    "    for batch_X, batch_Y in cifar10_train_stream.get_epoch_iterator():\n",
    "        feed_dict = {X: batch_X, Y: batch_Y}\n",
    "        _, global_step_v, loss_v = sess.run([train_op, global_step, batch_loss], feed_dict=feed_dict)\n",
    "        if (global_step_v % 1000) == 1:\n",
    "            train_loss_history.append((epoch, global_step_v, loss_v,))\n",
    "            print (\"epoch: %d, step: %d, loss: %g\" % (epoch, global_step_v, loss_v,))\n",
    "    \n",
    "    test_stats = []\n",
    "    for batch_X, batch_Y in cifar10_validation_stream.get_epoch_iterator():\n",
    "        feed_dict = {X: batch_X, Y: batch_Y}\n",
    "        classification_v, batch_loss_v = sess.run([classification, batch_loss], feed_dict=feed_dict)\n",
    "        batch_accuracy = np.mean(classification_v == batch_Y[:,0])\n",
    "        test_stats.append((batch_accuracy, batch_loss_v)) \n",
    "        \n",
    "    valid_acc, valid_batch_loss = np.mean(test_stats, axis=0)\n",
    "    print (\"epoch: %d, step: %d, valid_loss: %g, valid_acc: %f\" % (epoch, global_step_v, valid_batch_loss, valid_acc))\n",
    "    valid_loss_history.append((epoch, global_step_v, valid_batch_loss, valid_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy 0.7205\n"
     ]
    }
   ],
   "source": [
    "test_accuracy = []\n",
    "for batch_X, batch_Y in cifar10_test_stream.get_epoch_iterator():\n",
    "    feed_dict = {X: batch_X, Y: batch_Y}\n",
    "    classification_v = sess.run(classification, feed_dict=feed_dict)\n",
    "    batch_accuracy = np.mean(classification_v == batch_Y[:, 0])\n",
    "    test_accuracy.append(batch_accuracy)\n",
    "    \n",
    "print(\"test accuracy %g\" % np.mean(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape=[None, 3072])\n",
    "y_ = tf.placeholder(tf.int64, shape=[None, 10])\n",
    "W = tf.Variable(tf.zeros([3072, 10]))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "y = tf.matmul(x, W) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conv1_f = 32\n",
    "conv2_f = 64\n",
    "# conv3_f = 128\n",
    "fc1_w = 1024\n",
    "fc2_w = 10\n",
    "\n",
    "af_conv = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_image = tf.reshape(x, [-1, 32, 32, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 16, 16, 32)\n"
     ]
    }
   ],
   "source": [
    "W_conv1 = weight_variable([5, 5, 3, conv1_f])\n",
    "b_conv1 = bias_variable([conv1_f])\n",
    "\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "h_pool1 = max_pool_2x2(h_conv1)\n",
    "print h_pool1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 8, 8, 64)\n"
     ]
    }
   ],
   "source": [
    "W_conv2 = weight_variable([5, 5, conv1_f, conv2_f])\n",
    "b_conv2 = bias_variable([conv2_f])\n",
    "\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)\n",
    "print h_pool2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# W_conv3 = weight_variable([5, 5, conv2_f, conv3_f])\n",
    "# b_conv3 = bias_variable([conv3_f])\n",
    "\n",
    "# h_conv3 = tf.nn.relu(conv2d(h_pool2, W_conv3) + b_conv3)\n",
    "# h_pool3 = max_pool_2x2(h_conv3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "W_fc1 = weight_variable([af_conv * af_conv * conv2_f, fc1_w])\n",
    "b_fc1 = bias_variable([fc1_w])\n",
    "\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, af_conv * af_conv * conv2_f])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keep_prob = tf.placeholder(tf.float32)\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "W_fc2 = weight_variable([fc1_w, fc2_w])\n",
    "b_fc2 = bias_variable([fc2_w])\n",
    "\n",
    "y_conv = tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cross_entropy = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv)\n",
    ")\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# counter = 0\n",
    "\n",
    "# try:\n",
    "#     for b in cifar10_train_stream.get_epoch_iterator():\n",
    "#         print \"batch 1\"\n",
    "#         while 1:\n",
    "#             print 'n', next(b)\n",
    "#         counter += 1\n",
    "#     print counter\n",
    "# except:\n",
    "#     print counter, sys.exc_info()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, training accuracy 0.96\n",
      "     0, valid accuracy: 0.607\n",
      "step 100, training accuracy 0.76\n",
      "     100, valid accuracy: 0.6015\n",
      "step 200, training accuracy 0.8\n",
      "     200, valid accuracy: 0.6014\n",
      "step 300, training accuracy 0.84\n",
      "     300, valid accuracy: 0.6061\n",
      "step 400, training accuracy 0.76\n",
      "     400, valid accuracy: 0.6038\n",
      "step 500, training accuracy 0.8\n",
      "     500, valid accuracy: 0.61\n",
      "step 600, training accuracy 0.8\n",
      "     600, valid accuracy: 0.606\n",
      "step 700, training accuracy 0.8\n",
      "     700, valid accuracy: 0.6031\n",
      "step 800, training accuracy 0.76\n",
      "     800, valid accuracy: 0.6046\n",
      "step 900, training accuracy 0.76\n",
      "     900, valid accuracy: 0.606\n",
      "step 1000, training accuracy 0.84\n",
      "     1000, valid accuracy: 0.6013\n",
      "step 1100, training accuracy 0.92\n",
      "     1100, valid accuracy: 0.6031\n",
      "step 1200, training accuracy 0.96\n",
      "     1200, valid accuracy: 0.6054\n",
      "step 1300, training accuracy 0.8\n",
      "     1300, valid accuracy: 0.6063\n",
      "step 1400, training accuracy 0.92\n",
      "     1400, valid accuracy: 0.6037\n",
      "step 1500, training accuracy 0.92\n",
      "     1500, valid accuracy: 0.6024\n",
      "step 1600, training accuracy 0.84\n",
      "     1600, valid accuracy: 0.6036\n",
      "step 1700, training accuracy 0.84\n",
      "     1700, valid accuracy: 0.6091\n",
      "step 1800, training accuracy 0.84\n",
      "     1800, valid accuracy: 0.6028\n",
      "step 1900, training accuracy 0.8\n",
      "     1900, valid accuracy: 0.6087\n",
      "step 2000, training accuracy 0.84\n",
      "     2000, valid accuracy: 0.6092\n",
      "step 2100, training accuracy 0.72\n",
      "     2100, valid accuracy: 0.6045\n",
      "step 2200, training accuracy 0.84\n",
      "     2200, valid accuracy: 0.6096\n",
      "step 2300, training accuracy 0.96\n",
      "     2300, valid accuracy: 0.607\n",
      "step 2400, training accuracy 0.88\n",
      "     2400, valid accuracy: 0.6062\n",
      "step 2500, training accuracy 0.84\n",
      "     2500, valid accuracy: 0.6068\n",
      "step 2600, training accuracy 0.88\n",
      "     2600, valid accuracy: 0.611\n",
      "step 2700, training accuracy 0.8\n",
      "     2700, valid accuracy: 0.6069\n",
      "step 2800, training accuracy 0.92\n",
      "     2800, valid accuracy: 0.6107\n",
      "step 2900, training accuracy 0.92\n",
      "     2900, valid accuracy: 0.6111\n",
      "step 3000, training accuracy 0.96\n",
      "     3000, valid accuracy: 0.6107\n",
      "step 3100, training accuracy 0.84\n",
      "     3100, valid accuracy: 0.6083\n",
      "step 3200, training accuracy 0.92\n",
      "     3200, valid accuracy: 0.6057\n",
      "step 3300, training accuracy 0.88\n",
      "     3300, valid accuracy: 0.6006\n",
      "step 3400, training accuracy 0.92\n",
      "     3400, valid accuracy: 0.6025\n",
      "step 3500, training accuracy 0.8\n",
      "     3500, valid accuracy: 0.6082\n",
      "step 3600, training accuracy 0.84\n",
      "     3600, valid accuracy: 0.6088\n",
      "step 3700, training accuracy 0.8\n",
      "     3700, valid accuracy: 0.6032\n",
      "step 3800, training accuracy 0.84\n",
      "     3800, valid accuracy: 0.6086\n",
      "step 3900, training accuracy 0.72\n",
      "     3900, valid accuracy: 0.6041\n",
      "step 4000, training accuracy 0.84\n",
      "     4000, valid accuracy: 0.6118\n",
      "step 4100, training accuracy 0.96\n",
      "     4100, valid accuracy: 0.6134\n",
      "step 4200, training accuracy 0.88\n",
      "     4200, valid accuracy: 0.6099\n",
      "step 4300, training accuracy 0.76\n",
      "     4300, valid accuracy: 0.6122\n",
      "step 4400, training accuracy 0.88\n",
      "     4400, valid accuracy: 0.6041\n",
      "step 4500, training accuracy 0.88\n",
      "     4500, valid accuracy: 0.6137\n",
      "step 4600, training accuracy 0.84\n",
      "     4600, valid accuracy: 0.6075\n",
      "step 4700, training accuracy 0.8\n",
      "     4700, valid accuracy: 0.607\n",
      "step 4800, training accuracy 0.88\n",
      "     4800, valid accuracy: 0.6153\n",
      "step 4900, training accuracy 0.84\n",
      "     4900, valid accuracy: 0.6079\n",
      "step 5000, training accuracy 0.92\n",
      "     5000, valid accuracy: 0.6056\n",
      "step 5100, training accuracy 0.92\n",
      "     5100, valid accuracy: 0.609\n",
      "step 5200, training accuracy 0.88\n",
      "     5200, valid accuracy: 0.6104\n",
      "step 5300, training accuracy 0.84\n",
      "     5300, valid accuracy: 0.6097\n",
      "step 5400, training accuracy 0.92\n",
      "     5400, valid accuracy: 0.6131\n",
      "step 5500, training accuracy 0.8\n",
      "     5500, valid accuracy: 0.6111\n",
      "step 5600, training accuracy 0.72\n",
      "     5600, valid accuracy: 0.6122\n",
      "step 5700, training accuracy 0.88\n",
      "     5700, valid accuracy: 0.6081\n",
      "step 5800, training accuracy 0.96\n",
      "     5800, valid accuracy: 0.61\n",
      "step 5900, training accuracy 0.8\n",
      "     5900, valid accuracy: 0.6111\n",
      "step 6000, training accuracy 0.96\n",
      "     6000, valid accuracy: 0.6075\n",
      "step 6100, training accuracy 0.96\n",
      "     6100, valid accuracy: 0.6108\n",
      "step 6200, training accuracy 0.92\n",
      "     6200, valid accuracy: 0.6095\n",
      "step 6300, training accuracy 0.88\n",
      "     6300, valid accuracy: 0.6079\n",
      "step 6400, training accuracy 0.96\n",
      "     6400, valid accuracy: 0.6056\n",
      "step 6500, training accuracy 0.88\n",
      "     6500, valid accuracy: 0.6054\n",
      "step 6600, training accuracy 0.72\n",
      "     6600, valid accuracy: 0.6126\n",
      "step 6700, training accuracy 0.72\n",
      "     6700, valid accuracy: 0.6091\n",
      "step 6800, training accuracy 0.88\n",
      "     6800, valid accuracy: 0.6087\n",
      "step 6900, training accuracy 0.84\n",
      "     6900, valid accuracy: 0.6101\n",
      "step 7000, training accuracy 0.92\n",
      "     7000, valid accuracy: 0.611\n",
      "step 7100, training accuracy 0.84\n",
      "     7100, valid accuracy: 0.6085\n",
      "step 7200, training accuracy 1\n",
      "     7200, valid accuracy: 0.6107\n",
      "step 7300, training accuracy 0.96\n",
      "     7300, valid accuracy: 0.609\n",
      "step 7400, training accuracy 0.76\n",
      "     7400, valid accuracy: 0.6118\n",
      "step 7500, training accuracy 0.84\n",
      "     7500, valid accuracy: 0.6107\n",
      "step 7600, training accuracy 0.84\n",
      "     7600, valid accuracy: 0.6042\n",
      "step 7700, training accuracy 0.88\n",
      "     7700, valid accuracy: 0.6096\n",
      "step 7800, training accuracy 0.92\n",
      "     7800, valid accuracy: 0.6096\n",
      "step 7900, training accuracy 0.84\n",
      "     7900, valid accuracy: 0.6118\n",
      "step 8000, training accuracy 0.8\n",
      "     8000, valid accuracy: 0.6089\n",
      "step 8100, training accuracy 0.92\n",
      "     8100, valid accuracy: 0.609\n",
      "step 8200, training accuracy 0.96\n",
      "     8200, valid accuracy: 0.6105\n",
      "step 8300, training accuracy 0.68\n",
      "     8300, valid accuracy: 0.6151\n",
      "step 8400, training accuracy 0.8\n",
      "     8400, valid accuracy: 0.6065\n",
      "step 8500, training accuracy 0.84\n",
      "     8500, valid accuracy: 0.6118\n",
      "step 8600, training accuracy 0.88\n",
      "     8600, valid accuracy: 0.6099\n",
      "step 8700, training accuracy 0.96\n",
      "     8700, valid accuracy: 0.6074\n",
      "step 8800, training accuracy 0.8\n",
      "     8800, valid accuracy: 0.6071\n",
      "step 8900, training accuracy 0.88\n",
      "     8900, valid accuracy: 0.6077\n",
      "step 9000, training accuracy 0.84\n",
      "     9000, valid accuracy: 0.607\n",
      "step 9100, training accuracy 0.8\n",
      "     9100, valid accuracy: 0.6059\n",
      "step 9200, training accuracy 0.96\n",
      "     9200, valid accuracy: 0.6058\n",
      "step 9300, training accuracy 0.8\n",
      "     9300, valid accuracy: 0.6111\n",
      "step 9400, training accuracy 0.92\n",
      "     9400, valid accuracy: 0.6081\n",
      "step 9500, training accuracy 0.92\n",
      "     9500, valid accuracy: 0.6069\n",
      "step 9600, training accuracy 0.72\n",
      "     9600, valid accuracy: 0.6102\n",
      "step 9700, training accuracy 0.84\n",
      "     9700, valid accuracy: 0.6063\n",
      "step 9800, training accuracy 0.92\n",
      "     9800, valid accuracy: 0.6036\n",
      "step 9900, training accuracy 0.84\n",
      "     9900, valid accuracy: 0.6103\n"
     ]
    }
   ],
   "source": [
    "validation_errors = []\n",
    "\n",
    "for i in range(10000):\n",
    "    batch = next(cifar10_train_stream.get_epoch_iterator())\n",
    "    X = next(batch)\n",
    "    Y = next(batch)\n",
    "    # Y = np.array(np.arange(10, 100) == Y[0], dtype=np.uint8)\n",
    "    Y = np.array(np.arange(10) == Y, dtype=np.uint8)\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        train_accuracy = accuracy.eval(\n",
    "            feed_dict={x: X, y_: Y, keep_prob: 1.0}\n",
    "        )\n",
    "        print(\"step %d, training accuracy %g\" % (i, train_accuracy))\n",
    "        \n",
    "        val_acc = []\n",
    "        for v_X, v_Y in cifar10_validation_stream.get_epoch_iterator():\n",
    "            v_Y = np.array(np.arange(10) == v_Y, dtype=np.uint8)\n",
    "            acc = accuracy.eval(feed_dict={x: v_X, y_: v_Y, keep_prob: 1.0})\n",
    "            val_acc.append(acc)\n",
    "        print(\"     %d, valid accuracy: %g\" % (i, np.mean(val_acc)))\n",
    "        \n",
    "    train_step.run(feed_dict={x: X, y_: Y, keep_prob: 0.5})\n",
    "    \n",
    "    \n",
    "\n",
    "# print(\"test accuracy %g\" % accuracy.eval(\n",
    "#     feed_dict={x: cifar10.test.images, y_: cifar10.test.labels}\n",
    "# ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'accuracy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-9573ba2fa780>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcifar10_test_stream\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_epoch_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mtest_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m1.0\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[1;31m# print(\"step %d, test accuracy %g\" % (len(test_accuracy), test_acc))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mtest_accuracy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_acc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'accuracy' is not defined"
     ]
    }
   ],
   "source": [
    "test_accuracy = []\n",
    "for X, Y in cifar10_test_stream.get_epoch_iterator():\n",
    "    Y = np.array(np.arange(10) == Y, dtype=np.uint8)\n",
    "    test_acc = accuracy.eval(feed_dict={x: X, y_: Y, keep_prob: 1.0})\n",
    "    # print(\"step %d, test accuracy %g\" % (len(test_accuracy), test_acc))\n",
    "    test_accuracy.append(test_acc)\n",
    "    \n",
    "print(\"test accuracy %g\" % np.mean(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# subplot(2,1,1)\n",
    "# train_loss = np.array(train_loss)\n",
    "# semilogy(train_loss[:,0], train_loss[:,1], label='batch train loss')\n",
    "# legend()\n",
    "\n",
    "# subplot(2,1,2)\n",
    "# train_erros = np.array(train_erros)\n",
    "# plot(train_erros[:,0], train_erros[:,1], label='batch train error rate')\n",
    "# validation_errors = np.array(validation_errors)\n",
    "# plot(validation_errors[:,0], validation_errors[:,1], label='validation error rate', color='r')\n",
    "# ylim(0,0.2)\n",
    "# legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
