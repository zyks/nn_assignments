{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 4\n",
    "\n",
    "**Submission deadline: last classes before 13.04.2016**\n",
    "\n",
    "**Points: 11 + 1 bonus points**\n",
    "\n",
    "\n",
    "## Downloading this notebook\n",
    "\n",
    "This assignment is an Jupyter notebook. Download it by cloning https://github.com/janchorowski/nn_assignments. Follow the instructions in its README for instructions.\n",
    "\n",
    "For programming exerciese add your solutions to the notebook. For math exercies please provide us with answers on paper or type them in the notebook i supports Latex-like equations).\n",
    "\n",
    "Please do not hesitate to use GitHubâ€™s pull requests to send us corrections!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy.optimize as sopt\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn import metrics\n",
    "\n",
    "from common.gradients import check_gradient, numerical_gradient, encode_params, decode_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following assignments let:\n",
    " * $X \\in \\mathbb{R}^{k\\times N}$ be the data matrix containing $N$\n",
    "  samples each described with $k$ features. The $i$-th sample $x^{(i)} \\in\n",
    "  \\mathbb{R}^{(k\\times 1)}$ is the $i$-th column of $X$.\n",
    " * $Y \\in \\mathbb{R}^{1\\times N}$ be the row-vector of targets,\n",
    "  with $y^{(i)}$ being the target for the $i$-th sample.\n",
    " * $\\Theta\\in\\mathbb{R}^{k\\times 1}$ be the vector of parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1 (Backpropagation through a tanh neuron) [2p]\n",
    "\n",
    "  We want to use a single neuron with the $\\tanh(x) = \\frac{e^x -\n",
    "    e^{-x}}{e^x + e^{-x}}$ activation function.\n",
    "  First find the derivative $\\frac{\\partial \\tanh(x)}{\\partial\n",
    "    x}$ and express it as a function of $\\tanh(x)$.\n",
    "  Forward computations performed by the neuron are:\n",
    "  \n",
    "  \\begin{align*}\n",
    "    A &= \\Theta^T X \\\\\n",
    "    \\hat{Y} &= \\tanh(A) \\text{ applied elementwise} \\\\\n",
    "    E &= Y - \\hat{Y} \\\\\n",
    "    J &= E \\cdot E^T\n",
    "  \\end{align*}\n",
    "  \n",
    "  Find and express using matrix notation the following gradients. You\n",
    "  can refer to values and gradients computed earlier in the expressions for the\n",
    "  following ones -- just as you would when implementing a computer\n",
    "  program. Use $\\odot$ for the elementwise multiplication of matrices.\n",
    "\n",
    "  \\begin{align*}\n",
    "    \\frac{\\partial J}{\\partial E } &= ? \\\\\n",
    "    \\frac{\\partial J}{\\partial \\hat{Y}} &= ? \\\\\n",
    "    \\frac{\\partial J}{\\partial A} &= ? \\\\\n",
    "    \\frac{\\partial J}{\\partial \\Theta} &= ? \\\\\n",
    "  \\end{align*}\n",
    "  \n",
    "  **Note:** each gradient above should be implementable as a\n",
    "  compact expression in Python+NumPy.\n",
    "\n",
    "  **Hint:** write down the shapes of all values that you\n",
    "  compute. Work out the expressions for a single element of the\n",
    "  gradient, then see how they can be expressed using the matrix\n",
    "  notation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## SoftMax Regression\n",
    "\n",
    "The samples in the Iris dataset belong to one of three classes, while in\n",
    "CIFAR10 and MNIST they belong to one of 10 classes. Thus, linear regression cannot be\n",
    "applied because it distinguishes between two classes only.\n",
    "We will use SoftMax regression instead.\n",
    "\n",
    "Let $x\\in \\mathbb{R}^n$ be a sample vector and\n",
    "$y\\in \\{1,2,\\ldots,K\\}$ its class label.\n",
    "Similarly to what has been done during the lecture,\n",
    "we extend vector $x$ with the bias term $x_0=1$ to simplify the calculations\n",
    "(so now $x\\in \\mathbb{R}^{n+1}$).\n",
    "\n",
    "In SoftMax regression, we model conditional probability, that \n",
    "a given sample $x$ belongs to class $k$. Such model is parametrized\n",
    "with a matrix $\\Theta\\in\\mathbb{R}^{K \\times n+1}$.\n",
    "Note that in SoftMax regression, a separate linear model is build for each\n",
    "class. First we compute the vector $a$ of total inputs:\n",
    "\\begin{equation}\n",
    "a_k = \\sum_{j=0}^{n}\\Theta_{k,j}x_j,\n",
    "\\end{equation}\n",
    "or using matrix notation $a = \\Theta x$.\n",
    "Then we compute conditional probabilities using SoftMax regression:\n",
    "\\begin{equation}\n",
    "p(\\text{class}=k|x, \\Theta)= o_k = \\frac{\\exp{a_k}}{\\sum_{j=1}^K \\exp{a_j}}.\n",
    "\\end{equation}\n",
    "\n",
    "Function SoftMax transforms a $K$-element vector of real numbers\n",
    "to a vector of non-negative numbers which sum to 1. Thus they can be\n",
    "treated as probabilities assigned to $K$ separate classes.\n",
    "\n",
    "As it is the case with linear regression, we use cross-entropy\n",
    "as the loss function in SoftMax regression:\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "J^{(i)}(\\Theta) &= - \\sum_{k=1}^{K} [y^{(i)}=k]\\log o_k^{(i)} \\\\\n",
    "J(\\Theta) &= \\frac{1}{m}\\sum_{i=1}^m J^{(i)}(\\Theta)= -\\frac{1}{m}\\sum_{i=1}^m\\sum_{k=1}^{K} [y^{(i)}=k]\\log o_k^{(i)} \n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "where $[y^{(i)}=k]$ equals $1$ when the $i-$th sample belongs to class $k$,\n",
    "and $0$ otherwise.\n",
    "Value $[y^{(i)}=k]$ might be interpreted as the correct value of the $k$-th\n",
    "output of the model on sample $i$.\n",
    "In addition, the total loss is expressed as a mean loss of particular samples,\n",
    "to make it independent of the size of the training set.\n",
    "\n",
    "Loss function gradient with respect to total inputs $a$ is simple:\n",
    "\\begin{equation}\n",
    "    \\frac{\\partial J^{(i)}}{\\partial a^{(i)}_k} = o_k^{(i)} - [y^{(i)}=k].\n",
    "\\end{equation}\n",
    "\n",
    "Using the chain rule, the gradient of the loss function with respect to\n",
    "model parameters becomes:\n",
    "\\begin{equation}\n",
    "    \\frac{\\partial J}{\\partial \\Theta_{kj}} = \\sum_{i=1}^m \\frac{\\partial J}{\\partial J^{(i)}}\\frac{\\partial J^{(i)}}{\\partial \\Theta_{kj}} = \\sum_{i=1}^m \\frac{1}{m}\\cdot \\frac{\\partial J^{(i)}}{\\partial a^{(i)}_k} \\frac{\\partial a^{(i)}_k}{\\partial \\Theta_{kj}} = \\frac{1}{m}\\sum_{i=1}^m \\frac{\\partial J^{(i)}}{\\partial a^{(i)}_k} x^{(i)}_j.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2 [2p]\n",
    "  Implement SoftMax regression and apply it to the Iris dataset.\n",
    "  During training, use L-BFGS from `scipy.optimize`. You can initialize the algorithm\n",
    "  with a null matrix (all entries being zeros).\n",
    "  Obtained accuracy should be comparable with that of k-NN\n",
    "  (roughly 3% of errors).\n",
    "  If your model doesn't work, check the gradient using the `check_gradient`\n",
    "  routine from the Starter Code of Assignment 3, which computes the gradient numerically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Here we load the IRIS dataset.\n",
    "# We will create two datasets: one using all features, and one using just Petal Langth and Petal Width for visualizations\n",
    "#\n",
    "iris = datasets.load_iris()\n",
    "petal_length = iris.data[:,iris.feature_names.index('petal length (cm)')]\n",
    "petal_width = iris.data[:, iris.feature_names.index('petal width (cm)')]\n",
    "\n",
    "IrisXFull = np.vstack([np.ones_like(petal_length), iris.data.T])\n",
    "IrisX2feats = np.vstack([np.ones_like(petal_length), petal_length, petal_width])\n",
    "IrisY = iris.target.reshape(1,-1).astype(np.int64)\n",
    "\n",
    "print \"IrisXFull is a %s-shaped matrix of %s\" % (IrisXFull.shape, IrisXFull.dtype)\n",
    "print \"IrisX2feats is a %s-shaped matrix of %s\" % (IrisX2feats.shape, IrisX2feats.dtype)\n",
    "print \"IrisY is a %s-shaped matrix of %s\" % (IrisY.shape, IrisY.dtype)\n",
    "\n",
    "scatter(IrisX2feats[1,:], IrisX2feats[2,:], c=IrisY.ravel(), cmap='spring')\n",
    "xlabel('petal_length')\n",
    "ylabel('petal_width')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def SoftMaxRegression_implementation(ThetaFlat, X, Y=None, return_probabilities=False):\n",
    "    \"\"\"\n",
    "    Compute the outputs of a softmax classifier, or the loss and gradient\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ThetaFlat : \n",
    "        flat array of parameters containing (n_features*n_classes) entries\n",
    "    X :\n",
    "        array of features, shape n_features x n_smaples\n",
    "    Y :\n",
    "        optional array of desired targets of shape 1 x n_samples\n",
    "    return_probabilities : \n",
    "        if True, the probabilities are returned and Y is not used\n",
    "        if False, the los and gradient is computed on the X,Y pairs\n",
    "    \"\"\"\n",
    "    #X is num_features x num_samples\n",
    "    num_features, num_samples = X.shape\n",
    "\n",
    "    #Theta is num_features x num_classes\n",
    "    #we first reshape ThetaFlat into Theta\n",
    "    Theta = ThetaFlat.reshape(num_features, -1)\n",
    "\n",
    "    #Activation of softmax neurons\n",
    "    #A's shape should be num_classes x num_samples\n",
    "    #\n",
    "    # TODO \n",
    "    # A =\n",
    "    #\n",
    "    \n",
    "    #Now compute the SoftMax function\n",
    "    #O will be a num_classes x num_samples matrix of probabilities assigned by our model  \n",
    "    #Stability optimization - for each subtract the maximum activation\n",
    "    O = A - A.max(0, keepdims=True)\n",
    "    #\n",
    "    # TODO - compute SoftMax as vector O. Take the exp and normalize, so all values of O\n",
    "    #        would sum to 1.0.\n",
    "    #\n",
    "\n",
    "    if return_probabilities:\n",
    "        return O\n",
    "    \n",
    "    #The loss is the average per-sample nll (neg log likelihood)\n",
    "    #The nll is the sum of the logarithms of probabilities assigned to each class\n",
    "    correct_class_likelihoods = np.log(O[Y.ravel(), np.arange(num_samples)])\n",
    "    L = - 1.0/num_samples * np.sum(correct_class_likelihoods)\n",
    "\n",
    "    #For the softmax activation and cross-entropy loss, the derivative dNLL/dA has a simple form\n",
    "    #Please fill in its computation\n",
    "    #\n",
    "    # TODO\n",
    "    # dLdA = \n",
    "    #\n",
    "\n",
    "    #Now we compute the gradient of the loss with respect to Theta\n",
    "    dLdTheta = np.dot(X, dLdA.T)\n",
    "\n",
    "    #reshape gard into the shape of Theta, for fmin_l_bfsgb to work\n",
    "    return L, dLdTheta.reshape(ThetaFlat.shape)\n",
    "\n",
    "#Make a function for training on irises\n",
    "iris_log_reg_cost = lambda Theta: SoftMaxRegression_implementation(Theta, IrisXFull, IrisY, False)\n",
    "#Make sure that the gradient computation is OK\n",
    "check_gradient(iris_log_reg_cost, np.zeros((3*5,)))\n",
    "check_gradient(iris_log_reg_cost, np.random.rand(3*5)*2.0-1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Call a solver\n",
    "#\n",
    "\n",
    "#iprint will cause the solver to print TO THE TERMINAL from which ipython notebook was started\n",
    "ThetaOpt = sopt.fmin_l_bfgs_b(iris_log_reg_cost, np.zeros((3*5,)), iprint=1)[0]\n",
    "\n",
    "check_gradient(iris_log_reg_cost, ThetaOpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Compute training errors\n",
    "#\n",
    "\n",
    "probabilities = SoftMaxRegression_implementation(ThetaOpt, IrisXFull, return_probabilities=True)\n",
    "predictions = np.argmax(probabilities,0)\n",
    "\n",
    "print \"Training accurracy: %f%%\" % ((predictions==IrisY.ravel()).mean()*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Now redo the training for two features\n",
    "#\n",
    "# TODO - again, use l_bfgs to find optimal theta, then compute probabilities and new predictions.\n",
    "#\n",
    "\n",
    "print \"Training accurracy: %f%%\" % ((predictions==IrisY.ravel()).mean()*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Now plot the decision boundary\n",
    "# \n",
    "\n",
    "petal_lengths, petal_widths = np.meshgrid(np.linspace(IrisX2feats[1,:].min(), IrisX2feats[1,:].max(), 100),\n",
    "                                          np.linspace(IrisX2feats[2,:].min(), IrisX2feats[2,:].max(), 100))\n",
    "\n",
    "IrisXGrid = np.vstack([np.ones(np.prod(petal_lengths.shape)), petal_lengths.ravel(), petal_widths.ravel()])\n",
    "predictions_Grid = SoftMaxRegression_implementation(Theta2class, IrisXGrid, return_probabilities=True).argmax(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "contourf(petal_lengths, petal_widths, predictions_Grid.reshape(petal_lengths.shape), cmap='spring')\n",
    "scatter(IrisX2feats[1,:], IrisX2feats[2,:], c=IrisY.ravel(), cmap='spring')\n",
    "xlabel('petal_length')\n",
    "ylabel('petal_width')\n",
    "title('Decision boundary found by SoftMAx regression')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-layer Neural Network\n",
    "\n",
    "The task is to extend the SoftMax regression model to a 2-layer neural net.\n",
    "The network will transform an input vector to an activation vector\n",
    "of hidden neurons and finally, using the SoftMax function,\n",
    "to a vector of probabilities of the sample's belonging to one of 10 classes.\n",
    "\n",
    "To train the network, we'll need the loss function $J$ and its gradient\n",
    "with respect to network's parameters (weights and biases).\n",
    "For a 2-layer net, this can be achieved using the following relationships:\n",
    "\n",
    "### Data\n",
    "\n",
    "The training set has $m$ samples of $n$ dimensions, belonging to one\n",
    "of $K$ classes, it is given as a set of matrices: $X \\in \\mathbb{R}^{n\\times m}$\n",
    "and $Y\\in \\{1,2,\\ldots,K\\}^{1\\times m}$.\n",
    "\n",
    "### Parameters\n",
    "\n",
    "The net will have 2 layers: 1) a hidden one, having $L$ neurons,\n",
    "and 2) an output one, having $K$ neurons (one for each of $K$ classes).\n",
    "The layers are defined through:\n",
    "\n",
    "\n",
    "1. the parameters of the hidden layer, which maps $n$-dimensional input vectors\n",
    "  into activations of $L$ neurons:\n",
    "  weight matrix $W^h\\in\\mathbb{R}^{L\\times n}$ and bias vector\n",
    "  $b^h\\in\\mathbb{R}^{L\\times 1}$,\n",
    "  \n",
    "2. the parameters of the output layer, which maps $L$-dimensional vector\n",
    "  of activations of the hidden layer to $K$ activations of output neurons:\n",
    "  weight matrix $W^o\\in{K\\times L}$ and bias vector $b^o\\in\\mathbb{R}^{K\\times 1}$.\n",
    "\n",
    "### Signal forward propagation (fprop)\n",
    "\n",
    "Each hidden neuron computes its total input as a sum of product of its\n",
    "inputs, weight matrix and bias. For an $i$-th sample,\n",
    "the total input\n",
    "${a^{h}}^{(i)}_l $ of an $I$-th neuron is thus:\n",
    "\\begin{equation}\n",
    "{a^h}^{(i)}_l = \\sum_{j=1}^n {W^h}_{l,j}x^{(i)}_j + {b^h}_l\n",
    "\\end{equation}\n",
    "The total input of neurons might also be expressed via matrices,\n",
    "using matrix multiplication and broadcasting (which allows to add\n",
    "a column vector to all column vectors of a matrix):\n",
    "\\begin{equation}\n",
    "{a^h} = W^h\\cdot x + b^h\n",
    "\\end{equation}\n",
    "This can be implemented in Python as `ah = W.dot(x) + b`.\n",
    "\n",
    "Next, we compute activation $h^h$ of hidden neurons with hyperbolic tangent\n",
    "$\\tanh(a) = \\frac{e^a-e^{-a}}{e^a+e^{-a}}$:\n",
    "\\begin{equation}\n",
    "{h^h}^{(i)}_l=\\tanh({a^h}^{(i)}_l)\n",
    "\\end{equation}\n",
    "Thanks to vectorization in Python + numpy, $h^h$ might be computed with a single\n",
    "expression `hh = numpy.tanh(ah)`.\n",
    "\n",
    "Total input of the output layer can be computed using\n",
    "activations of the hidden layer (with the help of broadcasting) as:\n",
    "\n",
    "\\begin{equation}\n",
    "a^o = W^o\\cdot h^h + b^o\n",
    "\\end{equation}\n",
    "\n",
    "Finally, probabilities of a sample's belonging to  particular classes\n",
    "have to be computed. This can be achieved with SoftMax:\n",
    "\n",
    "\\begin{equation}\n",
    "    p(y^{(i)}=k|x^{(i)}) = o^{(i)}_k = \\frac{\\exp({a^o}^{(i)}_k)}{ \\sum_{k'=1}^K \\exp( {a^o}^{(i)}_{k'} )}.\n",
    "\\end{equation}\n",
    "\n",
    "Like with SoftMax regression, we will use cross-entropy\n",
    "as the loss function:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "J^{(i)}(\\Theta) &= - \\sum_{k=1}^{K} [y^{(i)}=k]\\log o_k^{(i)}, \\\\\n",
    "J(\\Theta) &= \\frac{1}{m}\\sum_{i=1}^m J^{(i)}(\\Theta)= -\\frac{1}{m}\\sum_{i=1}^n\\sum_{k=1}^{K} [y^{(i)}=k]\\log o_k^{(i)}.\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "### Error backpropagation (bprop)\n",
    "\n",
    "Using the chain rule one can derive the gradient of the loss function\n",
    "in respect to neurons' activations and network parameters.\n",
    "\n",
    "\n",
    "First we compute the gradient with respect to the output layer's\n",
    "total inputs:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\frac{\\partial J}{\\partial {a^o}^{(i)}_k} = \\frac{1}{m}(o_k^{(i)} - [y^{(i)}=k]),\n",
    "\\end{equation}\n",
    "\n",
    "then we compute the gradient with respect to activations of hidden units:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\frac{\\partial J}{\\partial {h^h}^{(i)}_l} = \\sum_{k=1}^K \\frac{\\partial J}{\\partial {a^o}^{(i)}_k} \\frac{\\partial {a^o}^{(i)}_k}{\\partial {h^h}^{(i)}_l} =  \\sum_{k=1}^K \\frac{\\partial J}{\\partial {a^o}^{(i)}_k} {W^o}_{kl},\n",
    "\\end{equation}\n",
    "then we compute the gradient with respect to the total activations of hidden units:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\frac{\\partial J}{\\partial {a^h}^{(i)}_l} = \\frac{\\partial J}{\\partial {h^h}^{(i)}_l}\\frac{\\partial {h^h}^{(i)}_l}{\\partial {a^h}^{(i)}_l} = \\frac{\\partial J}{\\partial {h^h}^{(i)}_l} (1 - ({h^h}^{(i)}_l)^2),\n",
    "\\end{equation}\n",
    "\n",
    "where we have used the relationship\n",
    "\n",
    "$\\frac{\\partial \\tanh(x)}{\\partial x} = 1-\\tanh(x)^2$.\n",
    "\n",
    "Finally we can use the gradients with respect to the total inputs to\n",
    "compute the gradients with respect to network parameters,\n",
    "eg. for the input layer:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\frac{\\partial J}{\\partial {W^o}_{kl}} = \\sum_{i}\\frac{\\partial J}{\\partial {a^o}^{(i)}_k}\\frac{\\partial {a^o}^{(i)}_k}{\\partial {W^o}_{kl}} = \\sum_{i}\\frac{\\partial J}{\\partial {a^o}^{(i)}_k}{h^h}^{(i)}_l,\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "    \\frac{\\partial J}{\\partial {b^o}_{k}} = \\sum_{i}\\frac{\\partial J}{\\partial {a^o}^{(i)}_k}\\frac{\\partial {a^o}^{(i)}_k}{\\partial {b^o}_{k}} = \\sum_{i}\\frac{\\partial J}{\\partial {a^o}^{(i)}_k}.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3 [2p]\n",
    "  Implement a 2-layer neural network as a function\n",
    "  **TwoLayerNet($\\Theta$,X,Y)**\n",
    "  which computes the loss and gradient of loss with\n",
    "  respect to the weights and bias terms (encoded as $\\Theta$)\n",
    "  on data given as $X$ and $Y$.\n",
    "  Refer to the Starter Code below for the details.\n",
    "  Try to express as much as possible with matrix calculus.\n",
    "\n",
    "  The following problems will require to train the network.\n",
    "  Use the L-BFGS optimizer from `scipy.optimize` to minimize\n",
    "  your function (particularly: the loss) and find the right $\\Theta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def TwoLayerNet_implementation(ThetaFlat, ThetaShapes, X, Y=None, return_probabilities=False):\n",
    "    \"\"\"\n",
    "    Compute the outputs of a softmax classifier, or the loss and gradient\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ThetaFlat : \n",
    "        flat array of parameters\n",
    "    ThetaShapes :\n",
    "        list of shapes of weight and bias matrices\n",
    "    X :\n",
    "        array of features, shape n_features x n_smaples\n",
    "    Y :\n",
    "        optional array of desired targets of shape 1 x n_samples\n",
    "    return_probabilities : \n",
    "        if True, the probabilities are returned and Y is not used\n",
    "        if False, the los and gradient is computed on the X,Y pairs\n",
    "    \"\"\"\n",
    "    #X is num_features x num_samples\n",
    "    num_features, num_samples = X.shape\n",
    "\n",
    "    #Extract weight matrices\n",
    "    W1, W2 = decode_params(ThetaFlat, ThetaShapes)\n",
    "    \n",
    "    X_padded = np.vstack([np.ones((1, num_samples)), X])\n",
    "    \n",
    "    #Activation in first layer. Shape is num_hidden x num_samples\n",
    "    #\n",
    "    # TODO\n",
    "    # A1 = \n",
    "    #\n",
    "\n",
    "    #Apply the transfer function\n",
    "    #\n",
    "    # TODO\n",
    "    # H1 = \n",
    "    #\n",
    "        \n",
    "    #Pad with zeros\n",
    "    H1_padded = np.vstack([np.ones((1, num_samples)), H1])\n",
    "    \n",
    "    #Now apply the second linear transform\n",
    "    #\n",
    "    # TODO\n",
    "    # A2 = \n",
    "    #\n",
    "    \n",
    "    #Now compute the SoftMax function\n",
    "    #O will be a num_classes x num_samples matrix of probabilities assigned by our model  \n",
    "    #Stability optimization - for each subtract the maximum activation\n",
    "    O = A2 - A2.max(0, keepdims=True)\n",
    "    # \n",
    "    # TODO - compute SoftMax as vector O. Take the exp and normalize, so all values of O\n",
    "    #        would sum to 1.0.\n",
    "    # \n",
    "\n",
    "    if return_probabilities:\n",
    "        return O\n",
    "    \n",
    "    #The loss is the average per-sample nll (neg log likelihood)\n",
    "    #The nll is the sum of the logarithms of probabilities assigned to each class\n",
    "    correct_class_likelihoods = np.log(O[Y.ravel(), np.arange(num_samples)])\n",
    "    L = - 1.0/num_samples * np.sum(correct_class_likelihoods)\n",
    "\n",
    "    #For the softmax activation and cross-entropy loss, the derivative dNLL/dA has a simple form\n",
    "    #Please fill in its computation\n",
    "    #\n",
    "    # TODO\n",
    "    # dLdA2 = \n",
    "    #\n",
    "\n",
    "    dLdH1_padded = W2.dot(dLdA2)\n",
    "    dLdH1 = dLdH1_padded[1:,:] #ship the derivatives backpropagated to the added ones\n",
    "    \n",
    "    #\n",
    "    # TODO - compute the derivatives dLdW2 and dLdW1\n",
    "    # Hint - to compute dLdW1, start with dLdA1\n",
    "    #\n",
    "    \n",
    "    dLdThetaFlat, unused_shapes = encode_params([dLdW1, dLdW2])\n",
    "    \n",
    "    #reshape gard into the shape of Theta, for fmin_l_bfsgb to work\n",
    "    return L, dLdThetaFlat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4 [1p] (choosing the initial vector)\n",
    "  In the cases of linear and logistic regression,\n",
    "  we could start the optimization with a vector of zeros.\n",
    "  Such initialization will be troublesome for neural networks.\n",
    "\n",
    "  You can use the following initialization methods for\n",
    "  network parameters: a) initialize weight matrices with small random\n",
    "  numbers (eg. drawn from $\\mathcal{N}(0, 0.2)$), b) initialize bias\n",
    "  vectors with zeros. Train the network on the Iris dataset and report\n",
    "  classification accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Here we init the network for gradient testing on IRIS\n",
    "#\n",
    "# We will have 7 hidden neurons.\n",
    "# The first weight matrix will be 5 (4 features + bias) x 7 (hidden neurons)\n",
    "# The second weight matrix will be 8 (7 neurons + bias) x 3 (classes)\n",
    "#\n",
    "num_hidden = 7\n",
    "#\n",
    "# TODO\n",
    "# W1 = \n",
    "# W2 = \n",
    "#\n",
    "\n",
    "# Now flatten into an array\n",
    "Theta0, ThetaShape = encode_params([W1,W2])\n",
    "\n",
    "#Make a function for training on irises\n",
    "iris_net_cost = lambda Theta: TwoLayerNet_implementation(Theta, ThetaShape, iris.data.T, IrisY, False)\n",
    "#Make sure that the gradient computation is OK\n",
    "check_gradient(iris_net_cost, Theta0)\n",
    "check_gradient(iris_net_cost, np.zeros_like(Theta0))\n",
    "check_gradient(iris_net_cost, np.ones_like(Theta0)*0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# TODO - apply L-BFGS to minimize the loss and get optimal ThetaOpt.\n",
    "#\n",
    "predictions = TwoLayerNet_implementation(ThetaOpt, ThetaShape, iris.data.T, return_probabilities=True).argmax(0)\n",
    "print \"Training accurracy: %f%%\" % ((predictions==IrisY.ravel()).mean()*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 5 [1p + 1p bonus]\n",
    "\n",
    "### XOR network\n",
    "Test your network on a 2-dimensional and a 3-dimensional\n",
    "XOR problem. How many hidden neurons the network requires to express\n",
    "the XOR function?\n",
    "\n",
    "### Iris network\n",
    "Normalize the Iris dataset, so that each of the 4 attributes\n",
    "would fall into {[}-1,1{]} interval. Train the network and check classification\n",
    "accuracy.\n",
    "\n",
    "### Bonus\n",
    "Plot samples (for XOR or for Iris) in hidden neurons' activation space\n",
    "(similarly to http://colah.github.io/posts/2014-03-NN-Manifolds-Topology/).\n",
    "\n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "XOR2X = np.array([[0,0],\n",
    "                  [0,1],\n",
    "                  [1,0],\n",
    "                  [1,1]]).T\n",
    "XOR2Y = np.array([[0,1,1,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#init the neurons\n",
    "num_hidden = 2\n",
    "W1 = (np.random.rand(3,num_hidden) - 0.5)\n",
    "W2 = (np.random.rand(num_hidden+1,2) -0.5)\n",
    "\n",
    "# Now flatten into an array\n",
    "Theta0, ThetaShape = encode_params([W1,W2])\n",
    "\n",
    "#\n",
    "# TODO - apply L-BFGS to minimize the loss and get optimal ThetaOpt.\n",
    "#\n",
    "\n",
    "TwoLayerNet_implementation(ThetaOpt, ThetaShape, XOR2X, return_probabilities=True).argmax(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# TODO - repeat the experiment for 3-dimensional XOR.\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# (Bonus)\n",
    "# TODO - change network implementation code to return hidden activations.\n",
    "# Hint - locals() gives the dictionary of all objects in a functions's scope!\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "IrisNormX = np.array(iris.data.T)\n",
    "#\n",
    "# TODO - normalize IrisNormX, so the vlaues would fall into [-1,1].\n",
    "#        Avoid looping constructs.\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_hidden = 10\n",
    "\n",
    "W1 = (np.random.rand(5, num_hidden) - 0.5)*0.1\n",
    "W2 = (np.random.rand(num_hidden + 1, 3) - 0.5)*0.1\n",
    "\n",
    "# Now flatten into an array\n",
    "Theta0, ThetaShape = encode_params([W1, W2])\n",
    "\n",
    "#\n",
    "# TODO - cripple and train your neural network.\n",
    "#\n",
    "\n",
    "predictions = TwoLayerNet_implementation(ThetaOpt, ThetaShape, IrisNormX, return_probabilities=True).argmax(0)\n",
    "print \"Training accurracy: %f%%\" % ((predictions==IrisY.ravel()).mean()*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 6 [1p]\n",
    "\n",
    "  Final questions:\n",
    "  \n",
    "   * Are neural networks parametric (https://en.wikipedia.org/wiki/Parametric_model) or non-parametric (https://en.wikipedia.org/wiki/Non-parametric_model) models? Why is it so?\n",
    "   \n",
    "   * What will happen if for each layer (hidden and output) all weights\n",
    "    will be initialized to the same values before the training?\n",
    "    \n",
    "   * How will the value of SoftMax function change,\n",
    "  if we will add the same constant term to each element of $a$?\n",
    "  Often, before computing SoftMax, the largest value can be subtracted\n",
    "  to mitigate large exponents and associated numerical errors.\n",
    "  Is it a good practice?\n",
    "\n",
    "   * Are two-class SoftMax regression and logistic regression equivalent (can you build a logistic regression model from a given softmax one and vice versa)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 7 [1p]\n",
    "You are provided with a starter code for a modular implementation of a\n",
    "feedforward neural network. Your general task is to fill in the blanks\n",
    "in the code, and validate the implementation on the IRIS task (ake sure\n",
    "that the network can be trained on the Iris dataset to reach 100% training\n",
    "accuracy.)\n",
    "\n",
    "**Note:** the next assignment list will involve using this code, or other neural \n",
    "network implementation that you will be able to fully explain to reach a low\n",
    "error rate on the mnist dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# These are taken from https://github.com/mila-udem/blocks\n",
    "# \n",
    "\n",
    "class Constant():\n",
    "    \"\"\"Initialize parameters to a constant.\n",
    "    The constant may be a scalar or a :class:`~numpy.ndarray` of any shape\n",
    "    that is broadcastable with the requested parameter arrays.\n",
    "    Parameters\n",
    "    ----------\n",
    "    constant : :class:`~numpy.ndarray`\n",
    "        The initialization value to use. Must be a scalar or an ndarray (or\n",
    "        compatible object, such as a nested list) that has a shape that is\n",
    "        broadcastable with any shape requested by `initialize`.\n",
    "    \"\"\"\n",
    "    def __init__(self, constant):\n",
    "        self._constant = numpy.asarray(constant)\n",
    "\n",
    "    def generate(self, rng, shape):\n",
    "        dest = numpy.empty(shape, dtype=np.float32)\n",
    "        dest[...] = self._constant\n",
    "        return dest\n",
    "\n",
    "\n",
    "class IsotropicGaussian():\n",
    "    \"\"\"Initialize parameters from an isotropic Gaussian distribution.\n",
    "    Parameters\n",
    "    ----------\n",
    "    std : float, optional\n",
    "        The standard deviation of the Gaussian distribution. Defaults to 1.\n",
    "    mean : float, optional\n",
    "        The mean of the Gaussian distribution. Defaults to 0\n",
    "    Notes\n",
    "    -----\n",
    "    Be careful: the standard deviation goes first and the mean goes\n",
    "    second!\n",
    "    \"\"\"\n",
    "    def __init__(self, std=1, mean=0):\n",
    "        self._mean = mean\n",
    "        self._std = std\n",
    "\n",
    "    def generate(self, rng, shape):\n",
    "        m = rng.normal(self._mean, self._std, size=shape)\n",
    "        return m.astype(np.float32)\n",
    "\n",
    "\n",
    "class Uniform():\n",
    "    \"\"\"Initialize parameters from a uniform distribution.\n",
    "    Parameters\n",
    "    ----------\n",
    "    mean : float, optional\n",
    "        The mean of the uniform distribution (i.e. the center of mass for\n",
    "        the density function); Defaults to 0.\n",
    "    width : float, optional\n",
    "        One way of specifying the range of the uniform distribution. The\n",
    "        support will be [mean - width/2, mean + width/2]. **Exactly one**\n",
    "        of `width` or `std` must be specified.\n",
    "    std : float, optional\n",
    "        An alternative method of specifying the range of the uniform\n",
    "        distribution. Chooses the width of the uniform such that random\n",
    "        variates will have a desired standard deviation. **Exactly one** of\n",
    "        `width` or `std` must be specified.\n",
    "    \"\"\"\n",
    "    def __init__(self, mean=0., width=None, std=None):\n",
    "        if (width is not None) == (std is not None):\n",
    "            raise ValueError(\"must specify width or std, \"\n",
    "                             \"but not both\")\n",
    "        if std is not None:\n",
    "            # Variance of a uniform is 1/12 * width^2\n",
    "            self._width = numpy.sqrt(12) * std\n",
    "        else:\n",
    "            self._width = width\n",
    "        self._mean = mean\n",
    "\n",
    "    def generate(self, rng, shape):\n",
    "        w = self._width / 2\n",
    "        m = rng.uniform(self._mean - w, self._mean + w, size=shape)\n",
    "        return m.astype(np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Layer(object):\n",
    "    def __init__(self, rng=None):\n",
    "        if rng is None:\n",
    "            rng = numpy.random\n",
    "        self.rng = rng\n",
    "    \n",
    "    @property\n",
    "    def parameters(self):\n",
    "        return []\n",
    "    \n",
    "    @property\n",
    "    def parameter_names(self):\n",
    "        return []\n",
    "    \n",
    "    def get_gradients(self, dLdY, fprop_context):\n",
    "        return []\n",
    "    \n",
    "\n",
    "class AffineLayer(Layer):\n",
    "    def __init__(self, num_in, num_out, weight_init=None, bias_init=None, **kwargs):\n",
    "        super(AffineLayer, self).__init__(**kwargs)\n",
    "        if weight_init is None:\n",
    "            #\n",
    "            # TODO propose a default initialization scheme.\n",
    "            # Type a sentence explaining why, and if you use a reference, \n",
    "            # cite it here\n",
    "            #\n",
    "            weight_init = TODO\n",
    "        if bias_init is None:\n",
    "            bias_init = Constant(0.0)\n",
    "        \n",
    "        self.W = weight_init.generate(self.rng, (num_out, num_in))\n",
    "        self.b = bias_init.generate(self.rng, (num_out, 1))\n",
    "    \n",
    "    @property\n",
    "    def parameters(self):\n",
    "        return [self.W, self.b]\n",
    "    \n",
    "    @property\n",
    "    def parameter_names(self):\n",
    "        return ['W','b']\n",
    "    \n",
    "    def fprop(self, X):\n",
    "        #Save X for later reusal\n",
    "        fprop_context = dict(X=X)\n",
    "        Y = np.dot(self.W, X) +  self.b\n",
    "        return Y, fprop_context\n",
    "    \n",
    "    def bprop(self, dLdY, fprop_context):\n",
    "        #\n",
    "        # TODO: fill in gradient computation\n",
    "        #\n",
    "        dLdX = TODO\n",
    "        return dLdX\n",
    "    \n",
    "    def get_gradients(self, dLdY, fprop_context):\n",
    "        X = fprop_context['X']\n",
    "        dLdW = np.dot(dLdY, X.T)\n",
    "        dLdb = dLdY.sum(1, keepdims=True)\n",
    "        return [dLdW, dLdb]\n",
    "    \n",
    "class TanhLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(TanhLayer, self).__init__(**kwargs)\n",
    "    \n",
    "    def fprop(self, X):\n",
    "        Y = np.tanh(X)\n",
    "        fprop_context = dict(Y=Y)\n",
    "        return Y, fprop_context\n",
    "    \n",
    "    def bprop(self, dLdY, fprop_context):\n",
    "        Y = fprop_context['Y']\n",
    "        #\n",
    "        # Fill in proper gradient computation\n",
    "        #\n",
    "        return TODO\n",
    "\n",
    "    \n",
    "class ReLULayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(ReLULayer, self).__init__(**kwargs)\n",
    "    \n",
    "    def fprop(self, X):\n",
    "        Y = np.maximum(X, 0.0)\n",
    "        fprop_context = dict(Y=Y)\n",
    "        return Y, fprop_context\n",
    "    \n",
    "    def bprop(self, dLdY, fprop_context):\n",
    "        Y = fprop_context['Y']\n",
    "        return dLdY * (Y>0)\n",
    "\n",
    "    \n",
    "class SoftMaxLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(SoftMaxLayer, self).__init__(**kwargs)\n",
    "    \n",
    "    def compute_probabilities(self, X):\n",
    "        O = X - X.max(axis=0, keepdims=True)\n",
    "        O = np.exp(O)\n",
    "        O /= O.sum(axis=0, keepdims=True)\n",
    "        return O\n",
    "    \n",
    "    def fprop_cost(self, X, Y):\n",
    "        NS = X.shape[1]\n",
    "        O = self.compute_probabilities(X)\n",
    "        Cost = -1.0/NS * np.log(O[Y.ravel(), range(NS)]).sum()\n",
    "        return Cost, O, dict(O=O, X=X, Y=Y)\n",
    "    \n",
    "    def bprop_cost(self, fprop_context):\n",
    "        X = fprop_context['X']\n",
    "        Y = fprop_context['Y']\n",
    "        O = fprop_context['O']\n",
    "        NS = X.shape[1]\n",
    "        dLdX = O.copy()\n",
    "        dLdX[Y, range(NS)] -= 1.0\n",
    "        dLdX /= NS\n",
    "        return dLdX\n",
    "    \n",
    "class FeedForwardNet(object):\n",
    "    def __init__(self, layers=None):\n",
    "        if layers is None:\n",
    "            layers = []\n",
    "        self.layers = layers\n",
    "    \n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "    \n",
    "    @property\n",
    "    def parameters(self):\n",
    "        params = []\n",
    "        for layer in self.layers:\n",
    "            params += layer.parameters\n",
    "        return params\n",
    "    \n",
    "    @parameters.setter\n",
    "    def parameters(self, values):\n",
    "        for ownP, newP in zip(self.parameters, values):\n",
    "            ownP[...] = newP\n",
    "    \n",
    "    @property\n",
    "    def parameter_names(self):\n",
    "        param_names = []\n",
    "        for layer in self.layers:\n",
    "            param_names += layer.parameter_names\n",
    "        return param_names\n",
    "    \n",
    "    def fprop(self, X):\n",
    "        for layer in self.layers[:-1]:\n",
    "            X, fp_context = layer.fprop(X)\n",
    "        return self.layers[-1].compute_probabilities(X)\n",
    "    \n",
    "    def get_cost_and_gradient(self, X, Y):\n",
    "        fp_contexts = []\n",
    "        for layer in self.layers[:-1]:\n",
    "            X, fp_context = layer.fprop(X)\n",
    "            fp_contexts.append(fp_context)\n",
    "        \n",
    "        L, O, fp_context = self.layers[-1].fprop_cost(X, Y)\n",
    "        dLdX = self.layers[-1].bprop_cost(fp_context)\n",
    "        \n",
    "        dLdP = [] #gradient with respect to parameters\n",
    "        for i in xrange(len(self.layers)-1):\n",
    "            layer = self.layers[len(self.layers)-2-i]\n",
    "            fp_context = fp_contexts[len(self.layers)-2-i]\n",
    "            dLdP = layer.get_gradients(dLdX, fp_context) + dLdP\n",
    "            dLdX = layer.bprop(dLdX, fp_context)\n",
    "        return L, O, dLdP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#training algorithms. They change the network!\n",
    "def GD(net, X, Y, alpha=1e-4, max_iters=1000000, tolerance=1e-6):\n",
    "    \"\"\"\n",
    "    Simple batch gradient descent\n",
    "    \"\"\"\n",
    "    old_L = np.inf\n",
    "    for i in xrange(max_iters):\n",
    "        L, O, gradients = net.get_cost_and_gradient(X, Y)\n",
    "        if old_L < L:\n",
    "            print \"Iter: %d, loss increased!!\" % (i,)\n",
    "        if (old_L - L)<tolerance:\n",
    "            print \"Tolerance level reached exiting\"\n",
    "            break\n",
    "        if i % 1000 == 0:\n",
    "            err_rate = (O.argmax(0) != Y).mean()\n",
    "            print \"At iteration %d, loss %f, train error rate %f%%\" % (i, L, err_rate*100)\n",
    "        for P,G in zip(net.parameters, gradients):\n",
    "            P -= alpha * G\n",
    "        old_L = L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "IrisX = iris.data.T\n",
    "IrisX = (IrisX - IrisX.mean(axis=1, keepdims=True)) / IrisX.std(axis=1, keepdims=True)\n",
    "IrisY = iris.target.reshape(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Here we verify that the network can be trained on Irises.\n",
    "# Most runs should result in 100% accurracy\n",
    "#\n",
    "\n",
    "net = FeedForwardNet([\n",
    "        AffineLayer(4,10),\n",
    "        TanhLayer(),\n",
    "        AffineLayer(10,3),\n",
    "        SoftMaxLayer()\n",
    "        ])\n",
    "GD(net, IrisX,IrisY, 1e-1, tolerance=1e-7, max_iters=50000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
